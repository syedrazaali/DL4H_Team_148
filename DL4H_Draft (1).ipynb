{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.word2vec.txt'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "metadata": {
        "id": "Zguc7rLQ33wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(img_dir)\n",
        "cv2.imshow(\"Title\", img)\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/My Drive/Deep Learning Data/'\n",
        "\n",
        "def load_csv(file_name):\n",
        "    # Function to load CSV and lower case column names\n",
        "    df = pd.read_csv(f'{data_dir}{file_name}', low_memory=False)\n",
        "    df.columns = df.columns.str.lower()  # Convert columns to lowercase\n",
        "    return df\n",
        "\n",
        "def load_data():\n",
        "    # Loading all the necessary datasets\n",
        "    datasets = {\n",
        "        # 'admissions': load_csv('admissions_filtered.csv'),\n",
        "        # 'chartevents': load_csv('filtered_CHARTEVENTS.csv'),\n",
        "        # 'labevents': load_csv('filtered_LABEVENTS.csv'),\n",
        "        # 'patients': load_csv('patients_filtered.csv'),\n",
        "        # 'icustays': load_csv('icustays_filtered.csv'),\n",
        "        # 'procedures_icd': load_csv('procedures_icd_full.csv'),\n",
        "        # 'diagnoses_icd': load_csv('diagnoses_icd_full.csv'),\n",
        "        'merged_data': load_csv('merged_data.csv')  # Loading the merged dataset\n",
        "    }\n",
        "    return datasets\n",
        "\n",
        "def calculate_stats(df):\n",
        "    print(\"\\nData Statistics:\")\n",
        "    print(f\"Total rows: {df.shape[0]}\")\n",
        "    print(f\"Total columns: {df.shape[1]}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    try:\n",
        "        print(df.describe())  # Simplified to ensure compatibility across different pandas versions\n",
        "        print(df.info())  # Provides data type for each column\n",
        "    except Exception as e:\n",
        "        print(\"Error in describing the data:\", e)\n",
        "\n",
        "# Load all datasets\n",
        "datasets = load_data()\n",
        "\n",
        "# Calculate statistics for all datasets to ensure consistency and understand the data\n",
        "for name, dataset in datasets.items():\n",
        "    print(f\"{name.upper()} Data:\")\n",
        "    calculate_stats(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_data_percentage(df):\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\n",
        "    percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending=False)\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    return missing_data\n",
        "\n",
        "# Load the merged data\n",
        "merged_data = datasets['merged_data']\n",
        "print(missing_data_percentage(merged_data))"
      ],
      "metadata": {
        "id": "aLLz82-e01tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a few instances where information has not been given based on the dataset we have crafted, going to do some imputation based handling for these cases to ensure the data is properly structured before model evaluations begin"
      ],
      "metadata": {
        "id": "Y-zH0X9t1tT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values more contextually\n",
        "merged_data['deathtime'].fillna('Not Applicable', inplace=True)  # Appropriate for non-existence of a death event\n",
        "\n",
        "# Assuming ICU-related missing data means no ICU stay. Fill time with admittime for continuity\n",
        "merged_data['icu_los'].fillna(0, inplace=True)  # Zero length for no ICU stay\n",
        "merged_data['icu_outtime'].fillna(merged_data['admittime'], inplace=True)  # Assuming no ICU stay ends at admission time\n",
        "merged_data['icu_intime'].fillna(merged_data['admittime'], inplace=True)  # Assuming no ICU stay starts at admission time\n",
        "merged_data['icustay_id'].fillna(-1, inplace=True)  # Use -1 as a placeholder for 'No ICU stay'\n",
        "\n",
        "# Diagnosis missing values are filled with 'Unknown'\n",
        "merged_data['diagnosis'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Convert date columns to datetime and handle potential errors\n",
        "date_columns = ['admittime', 'dischtime', 'icu_intime', 'icu_outtime']\n",
        "for col in date_columns:\n",
        "    merged_data[col] = pd.to_datetime(merged_data[col], errors='coerce')\n",
        "\n",
        "# Calculating lengths only after date imputations to avoid negative or zero values unexpectedly\n",
        "merged_data['hospital_stay_length'] = (merged_data['dischtime'] - merged_data['admittime']).dt.days.clip(lower=0)\n",
        "merged_data['icu_stay_length'] = (merged_data['icu_outtime'] - merged_data['icu_intime']).dt.total_seconds() / 86400\n",
        "merged_data['icu_stay_length'] = merged_data['icu_stay_length'].clip(lower=0)\n",
        "\n",
        "# Calculate time from admission to ICU\n",
        "merged_data['time_to_icu'] = (merged_data['icu_intime'] - merged_data['admittime']).dt.total_seconds() / 3600\n",
        "merged_data['time_to_icu'] = merged_data['time_to_icu'].clip(lower=0)\n",
        "\n",
        "# Check if any NaN values remain and print the updated stats\n",
        "print(merged_data.isnull().sum())\n",
        "\n",
        "# Check and correct gender inconsistencies\n",
        "print(\"Unique gender values before:\", merged_data['gender'].unique())\n",
        "merged_data['gender_male'] = (merged_data['gender'] == 'M').astype(int)\n",
        "print(\"Unique gender values after encoding:\", merged_data['gender_male'].unique())\n",
        "\n",
        "# Recalculate hospital and ICU stay lengths to correct potential errors\n",
        "merged_data['hospital_stay_length'] = (merged_data['dischtime'] - merged_data['admittime']).dt.days\n",
        "merged_data['icu_stay_length'] = (merged_data['icu_outtime'] - merged_data['icu_intime']).dt.total_seconds() / 86400\n",
        "merged_data['hospital_stay_length'] = merged_data['hospital_stay_length'].clip(lower=0)\n",
        "merged_data['icu_stay_length'] = merged_data['icu_stay_length'].clip(lower=0)\n",
        "\n",
        "# Create features\n",
        "merged_data['age_times_icu_length'] = merged_data['age_at_admission'] * merged_data['icu_stay_length']\n",
        "\n",
        "# Check new statistics after cleaning and feature engineering\n",
        "print(merged_data.describe())"
      ],
      "metadata": {
        "id": "ky4NG7Vk1Gkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing data that comes in from the noteevents data table from MIMIC 3 dataset"
      ],
      "metadata": {
        "id": "XSF3lVeuaHKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Stopwords and lemmatizer initialization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Removing stopwords and lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply text preprocessing on the 'all_notes' column\n",
        "merged_data['all_notes'] = merged_data['all_notes'].fillna('not available').apply(preprocess_text)\n",
        "\n",
        "# Example to check the preprocessing output\n",
        "print(merged_data['all_notes'].head())\n"
      ],
      "metadata": {
        "id": "BowwVxUOaGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed data to a CSV file\n",
        "data_dir = '/content/drive/My Drive/Deep Learning Data/'  # Specify your data directory\n",
        "merged_data.to_csv(data_dir + 'merged_data_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "LgNPgFaidHzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.read_csv(data_dir + 'merged_data_processed.csv')"
      ],
      "metadata": {
        "id": "Cs_3nWjrthi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the text data in the form of strings into numerical format that neural network can process. I'm going to be using word2vec to achieve this by doing the following\n",
        "\n",
        "1. Load pretrained embeddings\n",
        "2. Vecotrize the text\n",
        "3. Prepare text data for the model"
      ],
      "metadata": {
        "id": "ESXBkFde13Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you've already converted the GloVe file to Word2Vec format\n",
        "glove_input_file = data_dir + 'glove.6B.100d.txt'  # Update the file name if needed\n",
        "word2vec_output_file = data_dir + 'glove.6B.100d.word2vec.txt'  # Update the file name if needed\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# Load the GloVe model\n",
        "model = KeyedVectors.load_word2vec_format(glove_input_file, binary=False, no_header=True)\n",
        "\n",
        "# Function to vectorize a single note\n",
        "def vectorize_note(note, embedding_model):\n",
        "    vectors = [embedding_model[word] for word in note.split() if word in embedding_model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(embedding_model.vector_size)\n",
        "\n",
        "# Apply the vectorization to all notes and store in a list\n",
        "note_vectors = [vectorize_note(note, model) for note in merged_data['all_notes']]\n",
        "\n",
        "# Convert the list of vectors into a numpy array\n",
        "note_vectors_array = np.array(note_vectors)\n",
        "\n",
        "# Now, 'note_vectors_array' can be used as part of your input features"
      ],
      "metadata": {
        "id": "gw5S8Sgi12np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the array of vectors to a binary file in NumPy `.npy` format\n",
        "np.save(data_dir + 'note_vectors_array.npy', note_vectors_array)\n",
        "\n",
        "# load this array directly without reprocessing the text later to save time\n",
        "note_vectors_array = np.load(data_dir + 'note_vectors_array.npy')"
      ],
      "metadata": {
        "id": "JdYoM7X7G8-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis, before continuing further we're looking at the following:\n",
        "* Distribution Checks: Analyze the distribution of key metrics like hospital_stay_length, icu_stay_length, time_to_icu, and age_at_admission.\n",
        "* Correlation Analysis: Determine the relationships between the different features, particularly how various features like age, ICU stay, and hospital stay length correlate with the mortality label.\n",
        "* Visualization: Utilize histograms, box plots, scatter plots, and heatmaps to visually explore the data and uncover patterns or anomalies.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6OdgTGc-zrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Histograms\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.histplot(merged_data['hospital_stay_length'], bins=30, ax=axes[0], kde=True)\n",
        "axes[0].set_title('Histogram of Hospital Stay Length')\n",
        "sns.histplot(merged_data['icu_stay_length'], bins=30, ax=axes[1], kde=True)\n",
        "axes[1].set_title('Histogram of ICU Stay Length')\n",
        "sns.histplot(merged_data['age_at_admission'], bins=30, ax=axes[2], kde=True)\n",
        "axes[2].set_title('Histogram of Age at Admission')\n",
        "\n",
        "# Box plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.boxplot(x=merged_data['hospital_stay_length'], ax=axes[0])\n",
        "axes[0].set_title('Box Plot of Hospital Stay Length')\n",
        "sns.boxplot(x=merged_data['icu_stay_length'], ax=axes[1])\n",
        "axes[1].set_title('Box Plot of ICU Stay Length')\n",
        "sns.boxplot(x=merged_data['age_at_admission'], ax=axes[2])\n",
        "axes[2].set_title('Box Plot of Age at Admission')\n",
        "\n",
        "# Bar chart for gender\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='gender', data=merged_data)\n",
        "plt.title('Gender Distribution')\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(merged_data[['hospital_stay_length', 'icu_stay_length', 'age_at_admission', 'mortality_label']].corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ggRGkRTj_AbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Length Distribution\n",
        "merged_data['note_length'] = merged_data['all_notes'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(merged_data['note_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Note Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Common word analysis (could be more complex with NLP libraries for more insights)\n",
        "from collections import Counter\n",
        "all_words = Counter(\" \".join(merged_data['all_notes']).split())\n",
        "most_common_words = all_words.most_common(20)\n",
        "words, counts = zip(*most_common_words)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=list(words), y=list(counts))\n",
        "plt.title('Most Common Words in Clinical Notes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QR7BTVchp3hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'merged_data' and 'note_vectors_array.npy' are loaded\n",
        "note_vectors_array = np.load(data_dir + 'note_vectors_array.npy')\n",
        "\n",
        "# Define feature columns explicitly based on the previous descriptions and outputs\n",
        "feature_columns = [\n",
        "    'age_at_admission', 'icu_los', 'hospital_stay_length', 'time_to_icu',\n",
        "    'age_times_icu_length', 'gender_male'  # 'gender_male' added as it's created during preprocessing\n",
        "]\n",
        "\n",
        "# Prepare the numerical data\n",
        "numerical_features = merged_data[feature_columns].values.astype(np.float32)\n",
        "\n",
        "# Combine the numerical features and the note vectors\n",
        "combined_features = np.hstack((numerical_features, note_vectors_array))\n",
        "\n",
        "# Convert features and targets into torch tensors\n",
        "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
        "targets_tensor = torch.tensor(merged_data['mortality_label'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Split data into train and test sets\n",
        "features_train, features_test, targets_train, targets_test = train_test_split(\n",
        "    features_tensor, targets_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Dataset and DataLoader setup for both train and test sets\n",
        "train_dataset = TensorDataset(features_train, targets_train)\n",
        "test_dataset = TensorDataset(features_test, targets_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure x is 3D with sequence length of 1 if not provided\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)  # Add a sequence length dimension\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # Use the output of the last LSTM unit\n",
        "        return out\n",
        "\n",
        "# Setting up the model, loss function, and optimizer\n",
        "input_size = combined_features.shape[1]  # Make sure this matches your actual input feature size\n",
        "model = MyModel(input_size, 64, 1)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function for one epoch\n",
        "def train_model_one_epoch(model, train_loader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_func(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Execute the training loop\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model_one_epoch(model, train_loader, loss_func, optimizer)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted_classes = (torch.sigmoid(outputs) > 0.5).int()  # Convert probabilities to binary output\n",
        "            predictions.extend(predicted_classes.view(-1).cpu())\n",
        "            actuals.extend(targets.view(-1).cpu())\n",
        "\n",
        "    predictions = [p.item() for p in predictions]\n",
        "    actuals = [a.item() for a in actuals]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(actuals, predictions)\n",
        "    precision = precision_score(actuals, predictions)\n",
        "    recall = recall_score(actuals, predictions)\n",
        "    f1 = f1_score(actuals, predictions)\n",
        "    auc = roc_auc_score(actuals, predictions)\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Run evaluation\n",
        "accuracy, precision, recall, f1, auc = evaluate_model(model, test_loader)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(f\"AUC: {auc:.2f}\")"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example assuming you have stored train_loss values from each epoch in a list\n",
        "train_losses = [0.3011,  0.2474]  # Example losses from training epochs\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wgh37FD2bKq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}